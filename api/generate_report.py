# -*- coding: utf-8 -*-
"""
í†µí•© ë¦¬í¬íŠ¸ ìƒì„±ê¸° (ì£¼ê°„/ì›”ê°„ ë¦¬í¬íŠ¸ í†µí•©)
- ê¾¸ì¤€í•¨ ì§€ìˆ˜ + ì•„ì´ì½˜ + ì›”ê°„ êµ¬ì¡°í™” summary
- ê° ì‹¤íŒ¨ ì‚¬ìœ ë³„ {reason, icon} êµ¬ì¡° ì¶œë ¥
"""

import os
import json
import re
from datetime import datetime, timedelta
import requests
from collections import Counter
from dotenv import load_dotenv

# ===== í™˜ê²½ ë³€ìˆ˜ / ê²½ë¡œ ì„¤ì • =====
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={API_KEY}"

INPUT_DIR = "data"
OUTPUT_DIRS = {
    "weekly": "outputs/weekly_report",
    "monthly": "outputs/monthly_report",
}
for _d in OUTPUT_DIRS.values():
    os.makedirs(_d, exist_ok=True)

# ===== ê¾¸ì¤€í•¨ ì§€ìˆ˜ ë° ì´ëª¨ì§€ ë§¤í•‘ =====
CONSISTENCY_THRESHOLDS = {"high": 70, "medium": 40}
REASON_ICON_MAP = {
    "ì˜ì§€ ë¶€ì¡±": "ğŸ˜©",
    "ê±´ê°• ë¬¸ì œ": "ğŸ¤’",
    "ê³¼ë„í•œ ëª©í‘œ ì„¤ì •": "ğŸ¯",
    "ì‹œê°„ ë¶€ì¡±": "â°",
    "ì¼ì • ì¶©ëŒ": "ğŸ“…",
    "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)": "ğŸ’¬",
}

# ===== ììœ  ì…ë ¥ â†’ ì´ëª¨ì§€ ì¶”ë¡  =====
def guess_emoji_from_text(text: str) -> str:
    t = (text or "").lower().strip()
    if re.search(r"(ê·¼ìœ¡|í†µì¦|ìš´ë™|ì•„íŒŒ|í”¼ë¡œ|ëª¸|ìŠ¤íŠ¸ë ˆì¹­)", t): return "ğŸ’ª"
    if re.search(r"(ë¹„|rain|ì¥ë§ˆ|ìš°ì‚°)", t): return "ğŸŒ§ï¸"
    if re.search(r"(ë”ì›€|hot|heat|í­ì—¼|ë¥)", t): return "â˜€ï¸"
    if re.search(r"(ì¶”ì›€|cold|snow|í•œíŒŒ|ì¶¥)", t): return "ğŸ¥¶"
    if re.search(r"(í”¼ê³¤|ì¡¸|ìˆ˜ë©´|ì»¨ë””ì…˜|sleep|tired)", t): return "ğŸ˜´"
    if re.search(r"(ê³µë¶€|ì‹œí—˜|ìˆ™ì œ|ê³¼ì œ|project|work)", t): return "ğŸ“š"
    if re.search(r"(ì•½ì†|ì¹œêµ¬|ëª¨ì„|í–‰ì‚¬|íŒŒí‹°|ë°ì´íŠ¸|ë§Œë‚¨)", t): return "ğŸ§‘â€ğŸ¤â€ğŸ§‘"
    if re.search(r"(ì§€ê°|ì‹œê°„|ìŠ¤ì¼€ì¤„|ëŠ¦|ì¶œê·¼|ë“±êµ)", t): return "â°"
    if re.search(r"(ìš°ìš¸|ê¸°ë¶„|ì§œì¦|í™”|sad|depress)", t): return "ğŸ˜"
    return "ğŸ’¬"

# ===== ì‹œê°„ ìœ í‹¸ =====
def parse_hhmm(s: str):
    return datetime.combine(datetime.today().date(), datetime.strptime(s, "%H:%M").time())

def minutes_between(start_hhmm: str, end_hhmm: str) -> int:
    delta = parse_hhmm(end_hhmm) - parse_hhmm(start_hhmm)
    return max(int(delta.total_seconds() // 60), 0)

def extract_last_days_logs(logs, days: int):
    today = datetime.today().date()
    from_date = today - timedelta(days=days)
    return [log for log in logs if from_date <= datetime.strptime(log["date"], "%Y-%m-%d").date() <= today]

def minutes_filter_copy(habits, days: int):
    out = []
    for h in habits:
        out.append({
            "habit_id": h.get("habit_id"),
            "name": h.get("name"),
            "day_of_week": h.get("day_of_week", []),
            "start_time": h.get("start_time"),
            "end_time": h.get("end_time"),
            "habit_log": extract_last_days_logs(h.get("habit_log", []), days),
        })
    return out

# ===== ì‹¤íŒ¨ ì‚¬ìœ  ì •ê·œí™” =====
CATEGORY_RULES = [
    (r"(ì˜ìš•|ë™ê¸°\s*ì €í•˜|í•˜ê¸°\s*ì‹«|ë¯¸ë£¨|ê·€ì°®|ì˜ì§€\s*ë¶€ì¡±)", "ì˜ì§€ ë¶€ì¡±"),
    (r"(í”¼ê³¤|ìˆ˜ë©´\s*ë¶€ì¡±|ì¡¸ë¦¼|ëŠ¦ì |ì•ŒëŒ|ì»¨ë””ì…˜|ê°ê¸°|ë‘í†µ|í†µì¦|ê³¼ë¡œ)", "ê±´ê°• ë¬¸ì œ"),
    (r"(ê³¼ë„|ë¬´ë¦¬|ë²„ê²|ë¶€ë‹´|ê°•ë„\s*ë†’|ì‹œê°„\s*ë„ˆë¬´\s*ê¸¸|ë¹ˆë„\s*ì¦|ëª©í‘œ\s*í¬)", "ê³¼ë„í•œ ëª©í‘œ ì„¤ì •"),
    (r"(ì‹œê°„\s*ë¶€ì¡±|ë°”ì¨|ì—…ë¬´|íšŒì‚¬|ê³¼ì œ|ì‹œí—˜|ë§ˆê°|ì•Œë°”|ì¶œê·¼|ë“±êµ|ì§‘ì•ˆì¼)", "ì‹œê°„ ë¶€ì¡±"),
    (r"(ì¼ì •\s*ì¶©ëŒ|ì™¸ì¶œ|ì•½ì†|í–‰ì‚¬|ëª¨ì„|ì—¬í–‰|ì£¼ë§|ê³µíœ´ì¼)", "ì¼ì • ì¶©ëŒ"),
    (r"(ë‚ ì”¨|ë”ì›€|ì¶”ì›€|í­ì—¼|í­ìš°|ëˆˆ|í•œíŒŒ|ë¹„)", "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"),
]

def normalize_reason_category(text: str) -> str:
    t = (text or "").strip().lower()
    for pat, label in CATEGORY_RULES:
        if re.search(pat, t):
            return label
    return "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"

# ===== ê¾¸ì¤€í•¨ ì§€ìˆ˜ ê³„ì‚° =====
def compute_overall_success_rate(habits):
    attempts, successes = 0, 0
    for h in habits:
        logs = h.get("habit_log", [])
        attempts += len(logs)
        successes += sum(1 for log in logs if log.get("completed"))
    return (successes / attempts * 100) if attempts else 0.0

def consistency_level_from_rate(rate: float) -> str:
    if rate >= CONSISTENCY_THRESHOLDS["high"]:
        return "ë†’ìŒ"
    if rate >= CONSISTENCY_THRESHOLDS["medium"]:
        return "ë³´í†µ"
    return "ë‚®ìŒ"

# ===== ì‹¤íŒ¨ ì‚¬ìœ  ì§‘ê³„ =====
def compute_per_habit_top_failure_reasons(active_habits, topk: int = 2):
    result = []
    for habit in active_habits:
        hid = habit.get("habit_id")
        name = habit.get("name") or ""
        logs = habit.get("habit_log", [])
        counter = Counter()
        user_texts_for_etc = []

        # ì‚¬ìœ  ìˆ˜ì§‘
        for log in logs:
            if log.get("completed"):
                continue
            for raw in (log.get("failure_reason") or []):
                if not isinstance(raw, str) or not raw.strip():
                    continue
                label = normalize_reason_category(raw)
                counter[label] += 1
                if label == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
                    user_texts_for_etc.append(raw.strip())

        top_labels = [lbl for lbl, _ in counter.most_common(topk)]

        # ê¸°íƒ€ ì§ì ‘ì…ë ¥ â†’ ì›ë¬¸ ì¹˜í™˜
        final_reasons = []
        if "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" in top_labels and user_texts_for_etc:
            most_common_texts = [t for t, _ in Counter(user_texts_for_etc).most_common(topk)]
            for lbl in top_labels:
                if lbl == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
                    final_reasons.extend(most_common_texts)
                else:
                    final_reasons.append(lbl)
        else:
            final_reasons = top_labels

        final_reasons = final_reasons[:topk]

        # ê° ì´ìœ ë³„ ì´ëª¨ì§€
        reasons_with_icon = []
        for reason in final_reasons:
            normalized = normalize_reason_category(reason)
            icon = guess_emoji_from_text(reason) if normalized == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)" else REASON_ICON_MAP.get(normalized, "ğŸ’¬")
            reasons_with_icon.append({"reason": reason, "icon": icon})

        result.append({
            "habit_id": hid,
            "name": name,
            "reasons": reasons_with_icon,
        })

    return result

# ===== ìƒìœ„ ì‹¤íŒ¨ ì‚¬ìœ  í‰íƒ„í™” =====
def flatten_reasons_from_top_fail(failure_data):
    out = []
    for h in (failure_data or []):
        for r in h.get("reasons", []):
            if isinstance(r, dict) and "reason" in r:
                val = (r.get("reason") or "").strip()
                if val:
                    out.append(val)
    return out

# ===== ì›”ê°„ Summary ìƒì„± =====
def generate_monthly_summary(nickname, habits, failure_data):
    # 1) ì£¼ìš” ì„±ê³¼
    success_rates = []
    for h in habits:
        logs = h.get("habit_log", []) or []
        if not logs:
            continue
        success_rate = sum(1 for l in logs if l.get("completed")) / len(logs) * 100
        success_rates.append((h.get("name") or "ìŠµê´€", success_rate))
    if success_rates:
        best_habit, best_rate = max(success_rates, key=lambda x: x[1])
        consistency = f"{best_habit}ë„ ë°”ìœ í•œ ë‹¬ ì†ì—ì„œ {best_rate:.0f}%ë‚˜ í•´ëƒˆë‹¤ëŠ” ê±´ {nickname}ë‹˜ì˜ ê¾¸ì¤€í•¨ì´ ë‹ë³´ì…ë‹ˆë‹¤."
    else:
        consistency = "ì´ë²ˆ ë‹¬ì€ ìƒˆë¡œìš´ ì‹œì‘ì„ ìœ„í•œ ì¤€ë¹„ ê¸°ê°„ì´ì—ˆì–´ìš”."

    # 2) ì£¼ìš” ì‹¤íŒ¨ ì›ì¸
    all_reasons = flatten_reasons_from_top_fail(failure_data)
    if all_reasons:
        most_common_reason, _ = Counter(all_reasons).most_common(1)[0]
        normalized = normalize_reason_category(most_common_reason)
        if normalized == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
            top_texts = [txt for txt, _ in Counter(all_reasons).most_common(2)]
            icon = guess_emoji_from_text(top_texts[0]) if top_texts else "ğŸ’¬"
            if len(top_texts) == 1:
                failure_reasons = f"ì´ë²ˆ ë‹¬ì—” {icon} ì§ì ‘ ì…ë ¥ëœ ì‚¬ìœ ê°€ ë§ì•˜ì–´ìš”. ì˜ˆ: \"{top_texts[0]}\""
            else:
                failure_reasons = f"ì´ë²ˆ ë‹¬ì—” {icon} ì§ì ‘ ì…ë ¥ëœ ì‚¬ìœ ê°€ ë§ì•˜ì–´ìš”. ì˜ˆ: \"{top_texts[0]}\" / \"{top_texts[1]}\""
        else:
            icon = REASON_ICON_MAP.get(normalized, "ğŸ’¬")
            failure_reasons = f"ì´ë²ˆ ë‹¬ ê°€ì¥ ìì£¼ ë“±ì¥í•œ ë°©í•´ ìš”ì¸ì€ {icon} '{most_common_reason}'ì´ì—ìš”."
    else:
        failure_reasons = "ì´ë²ˆ ë‹¬ì€ í° ë°©í•´ ì—†ì´ ì˜ ì´ì–´ì¡Œì–´ìš”."

    # 3) ìš”ì¼ íŒ¨í„´
    weekday_success, weekday_total = Counter(), Counter()
    for h in habits:
        for log in h.get("habit_log", []) or []:
            try:
                day = datetime.strptime(log["date"], "%Y-%m-%d").weekday()
            except Exception:
                continue
            weekday_total[day] += 1
            if log.get("completed"):
                weekday_success[day] += 1
    if weekday_total:
        rate_by_day = {d: (weekday_success[d] / weekday_total[d]) for d in weekday_total}
        best_day = max(rate_by_day, key=rate_by_day.get)
        worst_day = min(rate_by_day, key=rate_by_day.get)
        day_map = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        daily_pattern = f"{day_map[best_day]}ìš”ì¼ì—” ë¦¬ë“¬ì´ ì¢‹ê³ , {day_map[worst_day]}ìš”ì¼ì—” ì•½ê°„ ëŠìŠ¨í–ˆì–´ìš”."
    else:
        daily_pattern = "ìš”ì¼ë³„ íŒ¨í„´ì„ í™•ì¸í•  ë°ì´í„°ê°€ ë¶€ì¡±í–ˆì–´ìš”."

    courage = "ì‘ì€ ê¾¸ì¤€í•¨ì´ ìŒ“ì—¬ ê²°êµ­ í° ë³€í™”ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ë‹¤ìŒ ë‹¬ì—ë„ ì‘ì›í• ê²Œìš”!"

    return {
        "consistency": consistency,
        "failure_reasons": failure_reasons,
        "daily_pattern": daily_pattern,
        "courage": courage
    }

# ===== ë©”ì¸ ë¡œì§ =====
def main():
    any_output = False
    for filename in os.listdir(INPUT_DIR):
        if not filename.endswith(".json"):
            continue

        path = os.path.join(INPUT_DIR, filename)
        try:
            data = json.load(open(path, "r", encoding="utf-8"))
        except Exception as e:
            print(f"âš ï¸ JSON ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
            continue

        user_id = data["user_id"]
        nickname = data.get("nickname", str(user_id))
        report_type = (data.get("type") or "monthly").lower()
        if report_type not in ("weekly", "monthly"):
            report_type = "monthly"

        filter_days = 7 if report_type == "weekly" else 30
        output_dir = OUTPUT_DIRS[report_type]
        habits_all = data.get("habits", [])
        active_habits = [h for h in minutes_filter_copy(habits_all, filter_days) if h.get("habit_log")]
        if not active_habits:
            continue

        end_date = datetime.today().date()
        start_date = end_date - timedelta(days=filter_days)
        parsed = {"start_date": str(start_date), "end_date": str(end_date)}

        # ì‹¤íŒ¨ ì‚¬ìœ  (ìƒìœ„ 2ê°œ)
        top_fail = compute_per_habit_top_failure_reasons(active_habits, topk=2)
        parsed["top_failure_reasons"] = top_fail

        # ì›”ê°„ summary + ê¾¸ì¤€í•¨ ì§€ìˆ˜
        if report_type == "monthly":
            rate = compute_overall_success_rate(active_habits)
            level = consistency_level_from_rate(rate)
            parsed["consistency_index"] = {
                "success_rate": round(rate, 1),
                "level": level,
                "thresholds": CONSISTENCY_THRESHOLDS,
                "display_message": f"ê¾¸ì¤€í•¨ ì§€ìˆ˜: {level}" + (" ğŸ”¥" if level == "ë†’ìŒ" else (" ğŸ™‚" if level == "ë³´í†µ" else " ğŸŒ§ï¸"))
            }
            parsed["summary"] = generate_monthly_summary(nickname, active_habits, top_fail)

        # ì €ì¥
        os.makedirs(output_dir, exist_ok=True)
        json_path = os.path.join(output_dir, f"user_{user_id}_{nickname}_{report_type}_report.json")
        json.dump(parsed, open(json_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {json_path}")
        any_output = True

    if not any_output:
        print("âš ï¸ ìƒì„± ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()